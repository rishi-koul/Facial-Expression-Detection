{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification = {0 : 'angry',\n",
    " 1 : 'disgust',\n",
    " 2 : 'fear',\n",
    " 3 : 'happy',\n",
    " 4 : 'neutral',\n",
    " 5 : 'sad',\n",
    " 6 : 'surprise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('finalized_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        x = x - 100\n",
    "        w = w + 200\n",
    "        y = y - 100\n",
    "        h = h + 200\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "            \n",
    "    roi_color = cv2.flip(roi_color,1)\n",
    "    return img\n",
    "\n",
    "def crop_image(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        x = x - 100\n",
    "        w = w + 200\n",
    "        y = y - 100\n",
    "        h = h + 200\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "            \n",
    "    roi_color = cv2.flip(roi_color,1)\n",
    "    return roi_color\n",
    "\n",
    "def generate_prediction(img):\n",
    "    try:       \n",
    "        img = cv2.resize(img, (48, 48), interpolation = cv2.INTER_CUBIC)\n",
    "        my_image_array = image.img_to_array(img)\n",
    "        save_img('my_image.jpg', my_image_array)\n",
    "        my_image = load_img('my_image.jpg', grayscale=True)\n",
    "\n",
    "        my_image_array = image.img_to_array(my_image)\n",
    "        my_image_array = np.expand_dims(my_image_array, axis = 0)\n",
    "        k = model.predict(my_image_array)\n",
    "        key = np.argmax(k)\n",
    "        return identification[key]\n",
    "    except cv2.error as e:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikoul/Desktop/ml-projects/ANN-prac/env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "angry\n",
      "angry\n",
      "angry\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    pred = generate_prediction(crop_image(frame))\n",
    "    print(pred)\n",
    "    cv2.putText(frame, pred, (75,290), cv2.FONT_HERSHEY_COMPLEX, 2, (100,170,0), 3)\n",
    "    cv2.imshow('Our Face Extractor', face_detector(frame))\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
